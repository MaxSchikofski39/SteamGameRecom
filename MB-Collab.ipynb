{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from surprise import SVD, Dataset, Reader\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "from typing import Tuple, List, Dict, Set\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendatios_df = pd.read_pickle('recommendations_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Neural Matrix Factorization (NeuMF) Model\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users: int, num_items: int, num_factors: int = 32,\n",
    "                 layers: List[int] = [64, 32, 16, 8], dropout: float = 0.1):\n",
    "        super(NeuMF, self).__init__()\n",
    "        \n",
    "        # GMF part\n",
    "        self.user_embedding_gmf = nn.Embedding(num_users, num_factors)\n",
    "        self.item_embedding_gmf = nn.Embedding(num_items, num_factors)\n",
    "        \n",
    "        # MLP part\n",
    "        self.user_embedding_mlp = nn.Embedding(num_users, num_factors)\n",
    "        self.item_embedding_mlp = nn.Embedding(num_items, num_factors)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp_layers = nn.ModuleList()\n",
    "        input_size = num_factors * 2\n",
    "        for layer_size in layers:\n",
    "            self.mlp_layers.append(nn.Linear(input_size, layer_size))\n",
    "            self.mlp_layers.append(nn.ReLU())\n",
    "            self.mlp_layers.append(nn.Dropout(dropout))\n",
    "            input_size = layer_size\n",
    "        \n",
    "        # Prediction layer\n",
    "        self.prediction = nn.Linear(layers[-1] + num_factors, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, user_input, item_input):\n",
    "        # GMF part\n",
    "        user_embedding_gmf = self.user_embedding_gmf(user_input)\n",
    "        item_embedding_gmf = self.item_embedding_gmf(item_input)\n",
    "        gmf_vector = user_embedding_gmf * item_embedding_gmf\n",
    "        \n",
    "        # MLP part\n",
    "        user_embedding_mlp = self.user_embedding_mlp(user_input)\n",
    "        item_embedding_mlp = self.item_embedding_mlp(item_input)\n",
    "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)\n",
    "        \n",
    "        for layer in self.mlp_layers:\n",
    "            mlp_vector = layer(mlp_vector)\n",
    "        \n",
    "        # Concatenate GMF and MLP parts\n",
    "        vector = torch.cat([gmf_vector, mlp_vector], dim=-1)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.prediction(vector)\n",
    "        return prediction.squeeze()\n",
    "\n",
    "class NeuMFRecommender:\n",
    "    def __init__(self, num_factors: int = 32, learning_rate: float = 0.001,\n",
    "                 batch_size: int = 256, epochs: int = 20, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.num_factors = num_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def fit(self, ratings_matrix: np.ndarray, user_mapping: Dict, item_mapping: Dict) -> None:\n",
    "        num_users, num_items = ratings_matrix.shape\n",
    "        \n",
    "        # Create model\n",
    "        self.model = NeuMF(num_users, num_items, self.num_factors).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Get non-zero ratings\n",
    "        user_indices, item_indices = np.nonzero(ratings_matrix)\n",
    "        ratings = ratings_matrix[user_indices, item_indices]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        user_indices = torch.LongTensor(user_indices).to(self.device)\n",
    "        item_indices = torch.LongTensor(item_indices).to(self.device)\n",
    "        ratings = torch.FloatTensor(ratings).to(self.device)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            num_batches = len(ratings) // self.batch_size\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * self.batch_size\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                \n",
    "                batch_users = user_indices[start_idx:end_idx]\n",
    "                batch_items = item_indices[start_idx:end_idx]\n",
    "                batch_ratings = ratings[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = self.model(batch_users, batch_items)\n",
    "                loss = criterion(predictions, batch_ratings)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / num_batches\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f'Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    def predict(self, user_idx: int, item_idx: int) -> float:\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.LongTensor([user_idx]).to(self.device)\n",
    "            item_tensor = torch.LongTensor([item_idx]).to(self.device)\n",
    "            prediction = self.model(user_tensor, item_tensor)\n",
    "            return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Main Recommender System Class\n",
    "class ModelBasedRecommender:\n",
    "    def __init__(self):\n",
    "        self.svd_model = None\n",
    "        self.neumf_model = None\n",
    "        self.als_model = None\n",
    "        self.pmf_model = None\n",
    "        self.user_mapping = None\n",
    "        self.item_mapping = None\n",
    "        self.reverse_user_mapping = None\n",
    "        self.reverse_item_mapping = None\n",
    "        self.ratings_matrix = None\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.test_df = None\n",
    "    \n",
    "    def setup_data(self, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Set up the data structures needed for the recommender system using pre-split data\"\"\"\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        \n",
    "        # Create user and item mappings from training data only\n",
    "        unique_users = train_df['user_id'].unique()\n",
    "        unique_items = train_df['app_id'].unique()\n",
    "        \n",
    "        self.user_mapping = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        self.item_mapping = {item: idx for idx, item in enumerate(unique_items)}\n",
    "        self.reverse_user_mapping = {idx: user for user, idx in self.user_mapping.items()}\n",
    "        self.reverse_item_mapping = {idx: item for item, idx in self.item_mapping.items()}\n",
    "        \n",
    "        # Create ratings matrix from training data\n",
    "        n_users = len(unique_users)\n",
    "        n_items = len(unique_items)\n",
    "        self.ratings_matrix = np.zeros((n_users, n_items))\n",
    "        \n",
    "        for _, row in train_df.iterrows():\n",
    "            if row['user_id'] in self.user_mapping and row['app_id'] in self.item_mapping:\n",
    "                user_idx = self.user_mapping[row['user_id']]\n",
    "                item_idx = self.item_mapping[row['app_id']]\n",
    "                self.ratings_matrix[user_idx, item_idx] = row['rating']\n",
    "    \n",
    "    def train_svd(self, n_factors: int = 100) -> None:\n",
    "        \"\"\"Train SVD model using surprise library.\"\"\"\n",
    "        if self.ratings_matrix is None:\n",
    "            raise ValueError(\"Must call setup_data before training models\")\n",
    "            \n",
    "        reader = Reader(rating_scale=(1, 5))\n",
    "        ratings_list = []\n",
    "        for user_idx in range(self.ratings_matrix.shape[0]):\n",
    "            for item_idx in range(self.ratings_matrix.shape[1]):\n",
    "                if self.ratings_matrix[user_idx, item_idx] > 0:\n",
    "                    ratings_list.append({\n",
    "                        'user': self.reverse_user_mapping[user_idx],\n",
    "                        'item': self.reverse_item_mapping[item_idx],\n",
    "                        'rating': self.ratings_matrix[user_idx, item_idx]\n",
    "                    })\n",
    "        \n",
    "        df = pd.DataFrame(ratings_list)\n",
    "        data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)\n",
    "        trainset = data.build_full_trainset()\n",
    "        \n",
    "        self.svd_model = SVD(n_factors=n_factors)\n",
    "        self.svd_model.fit(trainset)\n",
    "        \n",
    "    def train_als(self, factors: int = 100, regularization: float = 0.01, \n",
    "                  iterations: int = 15, alpha: float = 40) -> None:\n",
    "        \"\"\"Train Alternating Least Squares model using our own implementation.\"\"\"\n",
    "        if self.ratings_matrix is None:\n",
    "            raise ValueError(\"Must call setup_data before training models\")\n",
    "    \n",
    "        n_users, n_items = self.ratings_matrix.shape\n",
    "    \n",
    "        # Initialize random matrices\n",
    "        user_factors = np.random.random((n_users, factors))\n",
    "        item_factors = np.random.random((n_items, factors))\n",
    "    \n",
    "        # Get indices of non-zero entries\n",
    "        user_idx, item_idx = self.ratings_matrix.nonzero()\n",
    "        ratings = self.ratings_matrix[user_idx, item_idx]\n",
    "    \n",
    "        # Convert to confidence values\n",
    "        confidence = 1 + alpha * ratings\n",
    "    \n",
    "        # Alternate between updating user and item factors\n",
    "        for iteration in range(iterations):\n",
    "            # Update user factors\n",
    "            YtY = item_factors.T @ item_factors\n",
    "            for u in range(n_users):\n",
    "                # Get items rated by user u\n",
    "                items = self.ratings_matrix[u].nonzero()[0]\n",
    "                if len(items) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get confidence values for this user's ratings\n",
    "                Cu = confidence[user_idx == u]\n",
    "                Y_u = item_factors[items]\n",
    "            \n",
    "                # Compute user factors\n",
    "                A = YtY + np.dot(Y_u.T, (Cu[:, np.newaxis] - 1) * Y_u)\n",
    "                b = np.dot(Y_u.T, Cu * ratings[user_idx == u])\n",
    "                user_factors[u] = np.linalg.solve(A + regularization * np.eye(factors), b)\n",
    "        \n",
    "            # Update item factors\n",
    "            XtX = user_factors.T @ user_factors\n",
    "            for i in range(n_items):\n",
    "                # Get users who rated item i\n",
    "                users = self.ratings_matrix[:, i].nonzero()[0]\n",
    "                if len(users) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get confidence values for this item's ratings\n",
    "                Ci = confidence[item_idx == i]\n",
    "                X_i = user_factors[users]\n",
    "            \n",
    "                # Compute item factors\n",
    "                A = XtX + np.dot(X_i.T, (Ci[:, np.newaxis] - 1) * X_i)\n",
    "                b = np.dot(X_i.T, Ci * ratings[item_idx == i])\n",
    "                item_factors[i] = np.linalg.solve(A + regularization * np.eye(factors), b)\n",
    "        \n",
    "            if (iteration + 1) % 5 == 0:\n",
    "                print(f'Completed iteration {iteration + 1}/{iterations}')\n",
    "    \n",
    "        # Store the model components\n",
    "        self.als_model = {\n",
    "            'user_factors': user_factors,\n",
    "            'item_factors': item_factors\n",
    "        }\n",
    "        \n",
    "    def train_pmf(self, n_factors: int = 100, learning_rate: float = 0.005, \n",
    "                  regularization: float = 0.02, iterations: int = 50, \n",
    "                  batch_size: int = 1000) -> None:\n",
    "        \"\"\"Train Probabilistic Matrix Factorization model.\"\"\"\n",
    "        if self.ratings_matrix is None:\n",
    "            raise ValueError(\"Must call setup_data before training models\")\n",
    "    \n",
    "        n_users, n_items = self.ratings_matrix.shape\n",
    "    \n",
    "        # Initialize random matrices from normal distribution\n",
    "        self.user_factors = np.random.normal(0, 0.1, (n_users, n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (n_items, n_factors))\n",
    "    \n",
    "        # Get all non-zero ratings\n",
    "        user_indices, item_indices = self.ratings_matrix.nonzero()\n",
    "        ratings = self.ratings_matrix[user_indices, item_indices]\n",
    "        n_ratings = len(ratings)\n",
    "    \n",
    "        # Normalize ratings to [0, 1] range for numerical stability\n",
    "        ratings = (ratings - ratings.min()) / (ratings.max() - ratings.min())\n",
    "    \n",
    "        # Training loop\n",
    "        for iteration in range(iterations):\n",
    "            # Shuffle the data\n",
    "            idx = np.random.permutation(n_ratings)\n",
    "            user_indices = user_indices[idx]\n",
    "            item_indices = item_indices[idx]\n",
    "            ratings = ratings[idx]\n",
    "        \n",
    "            total_loss = 0\n",
    "        \n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_ratings, batch_size):\n",
    "                batch_users = user_indices[i:i + batch_size]\n",
    "                batch_items = item_indices[i:i + batch_size]\n",
    "                batch_ratings = ratings[i:i + batch_size]\n",
    "            \n",
    "                # Compute predictions\n",
    "                pred_ratings = np.sum(\n",
    "                    self.user_factors[batch_users] * self.item_factors[batch_items], \n",
    "                    axis=1\n",
    "                )\n",
    "            \n",
    "                # Compute gradients\n",
    "                error = batch_ratings - pred_ratings\n",
    "                total_loss += np.sum(error ** 2)\n",
    "            \n",
    "                # Update user factors\n",
    "                user_gradients = -2 * error[:, np.newaxis] * self.item_factors[batch_items] + \\\n",
    "                               2 * regularization * self.user_factors[batch_users]\n",
    "                self.user_factors[batch_users] -= learning_rate * user_gradients\n",
    "            \n",
    "                # Update item factors\n",
    "                item_gradients = -2 * error[:, np.newaxis] * self.user_factors[batch_users] + \\\n",
    "                               2 * regularization * self.item_factors[batch_items]\n",
    "                self.item_factors[batch_items] -= learning_rate * item_gradients\n",
    "        \n",
    "            # Print progress\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                rmse = np.sqrt(total_loss / n_ratings)\n",
    "                print(f'Iteration {iteration + 1}/{iterations}, RMSE: {rmse:.4f}')\n",
    "    \n",
    "        # Store the model\n",
    "        self.pmf_model = {\n",
    "            'user_factors': self.user_factors,\n",
    "            'item_factors': self.item_factors\n",
    "        }\n",
    "        \n",
    "    def train_neumf(self, n_factors: int = 32, learning_rate: float = 0.001,\n",
    "                   batch_size: int = 256, epochs: int = 20) -> None:\n",
    "        \n",
    "        if self.ratings_matrix is None:\n",
    "            raise ValueError(\"Must call setup_data before training models\")\n",
    "        \n",
    "        # Initialize NeuMF model\n",
    "        self.neumf_model = NeuMFRecommender(\n",
    "            num_factors=n_factors,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs\n",
    "        )\n",
    "    \n",
    "        # Train the model\n",
    "        self.neumf_model.fit(self.ratings_matrix, self.user_mapping, self.item_mapping)\n",
    "    \n",
    "    def get_recommendations(self, user_id: int, n_recommendations: int = 5, \n",
    "                          model_type: str = 'svd') -> List[Tuple[int, float]]:\n",
    "        \"\"\"Get top N recommendations for a user.\"\"\"\n",
    "        if user_id not in self.user_mapping:\n",
    "            return []\n",
    "        \n",
    "        if model_type == 'svd':\n",
    "            if self.svd_model is None:\n",
    "                raise ValueError(\"SVD model not trained\")\n",
    "            predictions = []\n",
    "            for item_id in self.item_mapping.keys():\n",
    "                pred = self.svd_model.predict(user_id, item_id).est\n",
    "                predictions.append((item_id, pred))\n",
    "        elif model_type == 'als':\n",
    "            if self.als_model is None:\n",
    "                raise ValueError(\"ALS model not trained\")\n",
    "            user_idx = self.user_mapping[user_id]\n",
    "            # Get scores for all items\n",
    "            scores = self.als_model['user_factors'][user_idx] @ self.als_model['item_factors'].T\n",
    "            # Create predictions list\n",
    "            predictions = [(self.reverse_item_mapping[i], score) \n",
    "                          for i, score in enumerate(scores)]\n",
    "            predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "            return predictions[:n_recommendations]\n",
    "        elif model_type == 'pmf':\n",
    "            if self.pmf_model is None:\n",
    "                raise ValueError(\"PMF model not trained\")\n",
    "            user_idx = self.user_mapping[user_id]\n",
    "            # Get scores for all items\n",
    "            scores = np.dot(self.pmf_model['user_factors'][user_idx], \n",
    "                           self.pmf_model['item_factors'].T)\n",
    "            # Create predictions list\n",
    "            predictions = [(self.reverse_item_mapping[i], score) \n",
    "                          for i, score in enumerate(scores)]\n",
    "        else:  # neumf\n",
    "            if self.neumf_model is None:\n",
    "                raise ValueError(\"NeuMF model not trained\")\n",
    "            user_idx = self.user_mapping[user_id]\n",
    "            predictions = []\n",
    "            for item_idx in range(len(self.item_mapping)):\n",
    "                pred = self.neumf_model.predict(user_idx, item_idx)\n",
    "                predictions.append((self.reverse_item_mapping[item_idx], pred))\n",
    "    \n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return predictions[:n_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_split(recommendations_df, test_size=0.15, val_size=0.15, timestamp_col='date'):\n",
    "    # Sort by the specified timestamp column if it exists\n",
    "    if timestamp_col in recommendations_df.columns:\n",
    "        recommendations_df = recommendations_df.sort_values(timestamp_col)\n",
    "    \n",
    "    # Group by user\n",
    "    user_groups = recommendations_df.groupby('user_id')\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for user_id, user_data in user_groups:\n",
    "        n_interactions = len(user_data)\n",
    "        \n",
    "        if n_interactions < 3:  # Need at least 3 interactions\n",
    "            continue\n",
    "            \n",
    "        # Split chronologically\n",
    "        n_test = max(1, int(n_interactions * test_size))\n",
    "        n_val = max(1, int(n_interactions * val_size))\n",
    "        \n",
    "        user_train = user_data.iloc[:-n_test-n_val]\n",
    "        user_val = user_data.iloc[-n_test-n_val:-n_test]\n",
    "        user_test = user_data.iloc[-n_test:]\n",
    "        \n",
    "        train_data.append(user_train)\n",
    "        val_data.append(user_val)\n",
    "        test_data.append(user_test)\n",
    "    \n",
    "    return pd.concat(train_data), pd.concat(val_data), pd.concat(test_data)\n",
    "\n",
    "# Create the splits\n",
    "train_df, val_df, test_df = create_train_val_test_split(recommendations_df)\n",
    "\n",
    "# Print split sizes\n",
    "print(f\"Train set: {len(train_df)} recommendations\")\n",
    "print(f\"Validation set: {len(val_df)} recommendations\")\n",
    "print(f\"Test set: {len(test_df)} recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_at_k(predictions: Dict[int, List[int]], \n",
    "                         ground_truth: Dict[int, Set[int]], \n",
    "                         k: int = 10) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, F1, and NDCG at k for recommendations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : Dict[int, List[int]]\n",
    "        Dictionary of user_id to list of recommended item_ids\n",
    "    ground_truth : Dict[int, Set[int]]\n",
    "        Dictionary of user_id to set of actual item_ids\n",
    "    k : int\n",
    "        Number of recommendations to consider\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[float, float, float, float]\n",
    "        (precision@k, recall@k, f1@k, ndcg@k)\n",
    "    \"\"\"\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    ndcg_list = []\n",
    "\n",
    "    for user in ground_truth:\n",
    "        if user not in predictions:\n",
    "            continue\n",
    "            \n",
    "        pred = list(predictions[user])[:k]\n",
    "        truth = ground_truth[user]\n",
    "        if not truth:\n",
    "            continue\n",
    "\n",
    "        # Calculate Precision and Recall\n",
    "        pred_set = set(pred)\n",
    "        intersection = pred_set & truth\n",
    "\n",
    "        precision = len(intersection) / k if len(pred) >= k else len(intersection) / len(pred)\n",
    "        recall = len(intersection) / len(truth)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        dcg = 0\n",
    "        for i, item in enumerate(pred):\n",
    "            rel = 1 if item in truth else 0\n",
    "            dcg += rel / np.log2(i + 2)  # i + 2 because i starts from 0\n",
    "            \n",
    "        # Calculate IDCG\n",
    "        ideal_ranking = [1] * min(len(truth), k)\n",
    "        idcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(ideal_ranking)])\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        \n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        ndcg_list.append(ndcg)\n",
    "\n",
    "    if not precision_list:\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    avg_precision = sum(precision_list) / len(precision_list)\n",
    "    avg_recall = sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = sum(f1_list) / len(f1_list)\n",
    "    avg_ndcg = sum(ndcg_list) / len(ndcg_list)\n",
    "\n",
    "    return avg_precision, avg_recall, avg_f1, avg_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and creating train-test split...\n",
      "Train set: 724198 recommendations\n",
      "Validation set: 155186 recommendations\n",
      "Test set: 155186 recommendations\n",
      "\n",
      "Training SVD model...\n",
      "\n",
      "Training ALS model...\n",
      "Completed iteration 5/15\n",
      "Completed iteration 10/15\n",
      "Completed iteration 15/15\n",
      "\n",
      "Training PMF model...\n",
      "Iteration 10/50, RMSE: 0.7649\n",
      "Iteration 20/50, RMSE: 0.3268\n",
      "Iteration 30/50, RMSE: 0.1949\n",
      "Iteration 40/50, RMSE: 0.1494\n",
      "Iteration 50/50, RMSE: 0.1257\n",
      "Training NeuMF model...\n",
      "Epoch 5/20, Loss: 0.0120\n",
      "Epoch 10/20, Loss: 0.0056\n",
      "Epoch 15/20, Loss: 0.0037\n",
      "Epoch 20/20, Loss: 0.0028\n",
      "\n",
      "Evaluating models on test set...\n",
      "\n",
      "Evaluating SVD model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting SVD predictions: 100%|██████████| 147226/147226 [03:38<00:00, 673.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.0005\n",
      "Recall@5: 0.0015\n",
      "F1@5: 0.0007\n",
      "NDCG@5: 0.0010\n",
      "\n",
      "Evaluating ALS model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting ALS predictions: 100%|██████████| 147226/147226 [00:32<00:00, 4487.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.0109\n",
      "Recall@5: 0.0495\n",
      "F1@5: 0.0176\n",
      "NDCG@5: 0.0271\n",
      "\n",
      "Evaluating PMF model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting PMF predictions: 100%|██████████| 147226/147226 [00:32<00:00, 4529.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.0015\n",
      "Recall@5: 0.0066\n",
      "F1@5: 0.0024\n",
      "NDCG@5: 0.0037\n",
      "\n",
      "Evaluating NEUMF model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting NEUMF predictions: 100%|██████████| 147226/147226 [3:29:14<00:00, 11.73it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.0021\n",
      "Recall@5: 0.0096\n",
      "F1@5: 0.0034\n",
      "NDCG@5: 0.0063\n",
      "\n",
      "Model Comparison:\n",
      "------------------------------------------------------------\n",
      "Model       Precision@k     Recall@k         F1@k       NDCG@k\n",
      "------------------------------------------------------------\n",
      "SVD              0.0005       0.0015       0.0007       0.0010\n",
      "ALS              0.0109       0.0495       0.0176       0.0271\n",
      "PMF              0.0015       0.0066       0.0024       0.0037\n",
      "NEUMF            0.0021       0.0096       0.0034       0.0063\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize recommender\n",
    "recommender = ModelBasedRecommender()\n",
    "\n",
    "# Load data and create train-test split\n",
    "print(\"Loading data and creating train-test split...\")\n",
    "with open('recommendations_processed.pkl', 'rb') as f:\n",
    "    ratings_data = pickle.load(f)\n",
    "\n",
    "# Create the splits using the baseline model's function\n",
    "train_df, val_df, test_df = create_train_val_test_split(ratings_data)\n",
    "print(f\"Train set: {len(train_df)} recommendations\")\n",
    "print(f\"Validation set: {len(val_df)} recommendations\")\n",
    "print(f\"Test set: {len(test_df)} recommendations\")\n",
    "\n",
    "# Setup the recommender with the data\n",
    "recommender.setup_data(train_df, val_df, test_df)\n",
    "\n",
    "# Train models\n",
    "print(\"\\nTraining SVD model...\")\n",
    "recommender.train_svd()\n",
    "\n",
    "print(\"\\nTraining ALS model...\")\n",
    "recommender.train_als()\n",
    "\n",
    "print(\"\\nTraining PMF model...\")\n",
    "recommender.train_pmf()\n",
    "\n",
    "print(\"Training NeuMF model...\")\n",
    "recommender.train_neumf()\n",
    "\n",
    "# Evaluate models\n",
    "print(\"\\nEvaluating models on test set...\")\n",
    "k = 5\n",
    "\n",
    "# Create ground truth from test set\n",
    "ground_truth = defaultdict(set)\n",
    "for _, row in test_df.iterrows():\n",
    "    if row['is_recommended'] == 1:\n",
    "        ground_truth[row['user_id']].add(row['app_id'])\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for model_type in ['svd', 'als', 'pmf', 'neumf']:\n",
    "    print(f\"\\nEvaluating {model_type.upper()} model:\")\n",
    "    predictions = defaultdict(list)\n",
    "    test_users = test_df['user_id'].unique()\n",
    "    \n",
    "    for user_id in tqdm(test_users, desc=f\"Getting {model_type.upper()} predictions\"):\n",
    "        if user_id not in recommender.user_mapping:\n",
    "            continue\n",
    "        recs = recommender.get_recommendations(user_id, n_recommendations=k, model_type=model_type)\n",
    "        predictions[user_id] = [rec[0] for rec in recs]\n",
    "    \n",
    "    precision, recall, f1, ndcg = calculate_metrics_at_k(predictions, ground_truth, k=k)\n",
    "    results[model_type] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'ndcg': ndcg\n",
    "    }\n",
    "    \n",
    "    print(f\"Precision@{k}: {precision:.4f}\")\n",
    "    print(f\"Recall@{k}: {recall:.4f}\")\n",
    "    print(f\"F1@{k}: {f1:.4f}\")\n",
    "    print(f\"NDCG@{k}: {ndcg:.4f}\")\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Model':<10} {'Precision@k':>12} {'Recall@k':>12} {'F1@k':>12} {'NDCG@k':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for model_type, metrics in results.items():\n",
    "    print(f\"{model_type.upper():<10} {metrics['precision']:>12.4f} {metrics['recall']:>12.4f} \"\n",
    "          f\"{metrics['f1']:>12.4f} {metrics['ndcg']:>12.4f}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
