{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from tqdm.contrib import tzip\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games DataFrame Shape: (4146, 7)\n",
      "Recommendations DataFrame Shape: (1034570, 8)\n"
     ]
    }
   ],
   "source": [
    "games_df = pd.read_pickle('games_processed.pkl')\n",
    "recommendations_df = pd.read_pickle('recommendations_processed.pkl')\n",
    "\n",
    "print(\"Games DataFrame Shape:\", games_df.shape)\n",
    "print(\"Recommendations DataFrame Shape:\", recommendations_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_features = games_df['tags'].fillna('')\n",
    "numeric_features = games_df[['price_final', 'rating', 'user_reviews']]\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(content_features)\n",
    "combined_features = hstack([tfidf_matrix, numeric_features])\n",
    "similarity_matrix = cosine_similarity(combined_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Content-Based System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_recommendations(game_id, similarity_matrix, games_df, n_recommendations=5):\n",
    "    game_idx = games_df[games_df['app_id'] == game_id].index[0]\n",
    "    similarity_scores = similarity_matrix[game_idx]\n",
    "    similar_indices = similarity_scores.argsort()[::-1][1:n_recommendations+1]\n",
    "    \n",
    "    recommendations = []\n",
    "    for idx in similar_indices:\n",
    "        game_id = games_df.iloc[idx]['app_id']\n",
    "        title = games_df.iloc[idx]['title']\n",
    "        similarity = similarity_scores[idx]\n",
    "        recommendations.append({\n",
    "            'game_id': game_id,\n",
    "            'title': title,\n",
    "            'similarity_score': similarity\n",
    "        })\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Collaborative System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collaborative_recommendations(user_id, recommendations_df, games_df, n_recommendations=5):\n",
    "    user_games = recommendations_df[recommendations_df['user_id'] == user_id]\n",
    "    user_positive_games = user_games[user_games['is_recommended'] == 1]['app_id'].tolist()\n",
    "    \n",
    "    if not user_positive_games:\n",
    "        return []\n",
    "    \n",
    "    similar_users = recommendations_df[\n",
    "        (recommendations_df['app_id'].isin(user_positive_games)) & \n",
    "        (recommendations_df['is_recommended'] == 1) &\n",
    "        (recommendations_df['user_id'] != user_id)\n",
    "    ]['user_id'].value_counts().head(5).index\n",
    "    \n",
    "    game_scores = {}\n",
    "    for similar_user in similar_users:\n",
    "        similar_user_games = recommendations_df[\n",
    "            (recommendations_df['user_id'] == similar_user) & \n",
    "            (recommendations_df['is_recommended'] == 1)\n",
    "        ]['app_id'].tolist()\n",
    "        \n",
    "        for game_id in similar_user_games:\n",
    "            if game_id not in user_positive_games:\n",
    "                if game_id not in game_scores:\n",
    "                    game_scores[game_id] = 0\n",
    "                game_scores[game_id] += 1\n",
    "    \n",
    "    recommendations = []\n",
    "    for game_id, score in sorted(game_scores.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]:\n",
    "        game_title = games_df[games_df['app_id'] == game_id]['title'].values[0]\n",
    "        recommendations.append({\n",
    "            'game_id': game_id,\n",
    "            'title': game_title,\n",
    "            'collab_score': score\n",
    "        })\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Content-Based and Collaborative Systems to build a Hybrid System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hybrid_recommendations(user_id, game_id, similarity_matrix, games_df, recommendations_df, \n",
    "                             n_recommendations=5, content_weight=0.6, collab_weight=0.4):\n",
    "\n",
    "    # Get content-based recommendations\n",
    "    content_recs = get_content_recommendations(game_id, similarity_matrix, games_df, n_recommendations*2)\n",
    "    content_scores = {rec['game_id']: rec['similarity_score'] for rec in content_recs}\n",
    "    \n",
    "    # Get collaborative recommendations\n",
    "    collab_recs = get_collaborative_recommendations(user_id, recommendations_df, games_df, n_recommendations*2)\n",
    "    collab_scores = {rec['game_id']: rec['collab_score'] for rec in collab_recs}\n",
    "    \n",
    "    # Combine scores\n",
    "    all_game_ids = set(content_scores.keys()) | set(collab_scores.keys())\n",
    "    hybrid_scores = {}\n",
    "    \n",
    "    # Min-max scaling for each set of scores\n",
    "    if content_scores:\n",
    "        max_content = max(content_scores.values())\n",
    "        min_content = min(content_scores.values())\n",
    "        content_range = max_content - min_content\n",
    "    \n",
    "    if collab_scores:\n",
    "        max_collab = max(collab_scores.values())\n",
    "        min_collab = min(collab_scores.values())\n",
    "        collab_range = max_collab - min_collab\n",
    "    \n",
    "    for game_id in all_game_ids:\n",
    "        # Normalize content score\n",
    "        if game_id in content_scores:\n",
    "            norm_content = (content_scores[game_id] - min_content) / content_range if content_range > 0 else 0\n",
    "        else:\n",
    "            norm_content = 0\n",
    "            \n",
    "        # Normalize collab score\n",
    "        if game_id in collab_scores:\n",
    "            norm_collab = (collab_scores[game_id] - min_collab) / collab_range if collab_range > 0 else 0\n",
    "        else:\n",
    "            norm_collab = 0\n",
    "            \n",
    "        # Calculate hybrid score\n",
    "        hybrid_scores[game_id] = (content_weight * norm_content + collab_weight * norm_collab)\n",
    "    \n",
    "    # Sort by hybrid score and get top N\n",
    "    sorted_games = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "    \n",
    "    # Format recommendations\n",
    "    recommendations = []\n",
    "    for game_id, score in sorted_games:\n",
    "        title = games_df[games_df['app_id'] == game_id]['title'].iloc[0]\n",
    "        recommendations.append({\n",
    "            'game_id': game_id,\n",
    "            'title': title\n",
    "        })\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 78304 recommendations\n",
      "Validation set: 36177 recommendations\n",
      "Test set: 36177 recommendations\n"
     ]
    }
   ],
   "source": [
    "def create_train_val_test_split(recommendations_df, test_size=0.15, val_size=0.15, timestamp_col='date'):\n",
    "\n",
    "    if timestamp_col in recommendations_df.columns:\n",
    "        recommendations_df = recommendations_df.sort_values(timestamp_col)\n",
    "    \n",
    "    # Group by user\n",
    "    user_groups = recommendations_df.groupby('user_id')\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for user_id, user_data in user_groups:\n",
    "        n_interactions = len(user_data)\n",
    "        \n",
    "        if n_interactions < 3:  # Need at least 3 interactions\n",
    "            continue\n",
    "            \n",
    "        # Split chronologically\n",
    "        n_test = max(1, int(n_interactions * test_size))\n",
    "        n_val = max(1, int(n_interactions * val_size))\n",
    "        \n",
    "        user_train = user_data.iloc[:-n_test-n_val]\n",
    "        user_val = user_data.iloc[-n_test-n_val:-n_test]\n",
    "        user_test = user_data.iloc[-n_test:]\n",
    "        \n",
    "        train_data.append(user_train)\n",
    "        val_data.append(user_val)\n",
    "        test_data.append(user_test)\n",
    "    \n",
    "    return pd.concat(train_data), pd.concat(val_data), pd.concat(test_data)\n",
    "\n",
    "train_df, val_df, test_df = create_train_val_test_split(recommendations_df)\n",
    "\n",
    "print(f\"Train set: {len(train_df)} recommendations\")\n",
    "print(f\"Validation set: {len(val_df)} recommendations\")\n",
    "print(f\"Test set: {len(test_df)} recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random User ID: 9282334\n",
      "Random Game ID: 972660\n",
      "Random Game Title: Spiritfarer®: Farewell Edition\n",
      "\n",
      "Content-Based Recommendations:\n",
      "1. Spiritfarer®: Farewell Edition - Digital Artbook (Similarity: 0.8998)\n",
      "2. HipWitch (Similarity: 0.7094)\n",
      "3. With You (Similarity: 0.7073)\n",
      "4. Chicory: A Colorful Tale (Similarity: 0.7063)\n",
      "5. Big Farm Story (Similarity: 0.7025)\n",
      "\n",
      "Collaborative Recommendations:\n",
      "1. Goose Goose Duck (Score: 2)\n",
      "2. Eastward (Score: 2)\n",
      "3. Superliminal (Score: 2)\n",
      "4. Unrailed! (Score: 2)\n",
      "5. When The Past Was Around (Score: 2)\n",
      "\n",
      "Hybrid Recommendations:\n",
      "1. Spiritfarer®: Farewell Edition - Digital Artbook\n",
      "2. Superliminal\n",
      "3. Goose Goose Duck\n",
      "4. When The Past Was Around\n",
      "5. Eastward\n"
     ]
    }
   ],
   "source": [
    "# Get a random user who has recommended games (is_recommended == 1)\n",
    "users_with_recommendations = test_df[test_df['is_recommended'] == 1]['user_id'].unique()\n",
    "sample_user = random.choice(users_with_recommendations)\n",
    "\n",
    "# Get a random game that this user has recommended\n",
    "user_recommended_games = test_df[(test_df['user_id'] == sample_user) & \n",
    "                               (test_df['is_recommended'] == 1)]['app_id'].values\n",
    "sample_game = random.choice(user_recommended_games)\n",
    "sample_game_title = games_df[games_df['app_id'] == sample_game]['title'].iloc[0]\n",
    "\n",
    "print(f\"Random User ID: {sample_user}\")\n",
    "print(f\"Random Game ID: {sample_game}\")\n",
    "print(f\"Random Game Title: {sample_game_title}\\n\")\n",
    "\n",
    "print(\"Content-Based Recommendations:\")\n",
    "content_recs = get_content_recommendations(sample_game, similarity_matrix, games_df)\n",
    "for i, rec in enumerate(content_recs, 1):\n",
    "    print(f\"{i}. {rec['title']} (Similarity: {rec['similarity_score']:.4f})\")\n",
    "\n",
    "print(\"\\nCollaborative Recommendations:\")\n",
    "collab_recs = get_collaborative_recommendations(sample_user, recommendations_df, games_df)\n",
    "for i, rec in enumerate(collab_recs, 1):\n",
    "    print(f\"{i}. {rec['title']} (Score: {rec['collab_score']})\")\n",
    "\n",
    "print(\"\\nHybrid Recommendations:\")\n",
    "hybrid_recs = get_hybrid_recommendations(sample_user, sample_game, similarity_matrix, games_df, recommendations_df)\n",
    "for i, rec in enumerate(hybrid_recs, 1):\n",
    "    print(f\"{i}. {rec['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(recommended_items, relevant_items, k):\n",
    "\n",
    "    recommended_items = recommended_items[:k]\n",
    "    relevant_items = set(relevant_items)  # Convert to set \n",
    "    \n",
    "    # Precision@k\n",
    "    hits = len(set(recommended_items) & relevant_items)\n",
    "    precision = hits / k if k > 0 else 0\n",
    "    \n",
    "    # Recall@k\n",
    "    recall = hits / len(relevant_items) if len(relevant_items) > 0 else 0\n",
    "    \n",
    "    # F1@k\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # NDCG@k calculation\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    \n",
    "    for i, item in enumerate(recommended_items):\n",
    "        rel = 1 if item in relevant_items else 0\n",
    "        dcg += rel / np.log2(i + 2)  \n",
    "    \n",
    "    n_rel = min(len(relevant_items), k)\n",
    "    for i in range(n_rel):\n",
    "        idcg += 1 / np.log2(i + 2)\n",
    "    \n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'ndcg': ndcg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_content_based_system(df, similarity_matrix, games_df, k=5):\n",
    "\n",
    "    metrics_sum = {'precision': 0, 'recall': 0, 'f1': 0, 'ndcg': 0}\n",
    "    total_users = 0\n",
    "    \n",
    "    # Get total number of valid users for progress bar\n",
    "    valid_users = sum(1 for _, user_data in df.groupby('user_id') \n",
    "                     if len(user_data[user_data['is_recommended'] == 1]['app_id']) >= 0)\n",
    "    \n",
    "    pbar = tqdm(total=valid_users, desc='Content-Based Evaluation', position=0)\n",
    "    \n",
    "    for user_id, user_data in df.groupby('user_id'):\n",
    "        user_liked_games = user_data[user_data['is_recommended'] == 1]['app_id'].tolist()\n",
    "        if len(user_liked_games) < 2:\n",
    "            continue\n",
    "            \n",
    "        input_game = user_liked_games[0]\n",
    "        test_games = set(user_liked_games[1:])\n",
    "        \n",
    "        try:\n",
    "            recs = get_content_recommendations(input_game, similarity_matrix, games_df, n_recommendations=k)\n",
    "            rec_ids = [rec['game_id'] for rec in recs]\n",
    "            \n",
    "            metrics = calculate_metrics(rec_ids, test_games, k)\n",
    "            for metric, value in metrics.items():\n",
    "                metrics_sum[metric] += value\n",
    "            total_users += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    avg_metrics = {\n",
    "        metric: value/total_users if total_users > 0 else 0 \n",
    "        for metric, value in metrics_sum.items()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nContent-Based Metrics:\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"{metric.upper()}@{k}: {value:.4f}\")\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_collaborative_system(train_df, test_df, games_df, k=5):\n",
    " \n",
    "    metrics_sum = {'precision': 0, 'recall': 0, 'f1': 0, 'ndcg': 0}\n",
    "    total_users = 0\n",
    "    \n",
    "    # Get test set ground truth\n",
    "    print(\"Building test set ground truth...\")\n",
    "    test_user_likes = defaultdict(set)\n",
    "    for _, row in tqdm(test_df[test_df['is_recommended'] == 1].iterrows(), \n",
    "                      desc='Processing test data', total=len(test_df[test_df['is_recommended'] == 1])):\n",
    "        test_user_likes[row['user_id']].add(row['app_id'])\n",
    "\n",
    "    users_in_test = test_df['user_id'].unique()\n",
    "    pbar = tqdm(total=len(users_in_test), desc='Collaborative Evaluation')\n",
    "\n",
    "    for user_id in users_in_test:\n",
    "        recs = get_collaborative_recommendations(user_id, train_df, games_df, n_recommendations=k)\n",
    "        recommended_games = [rec['game_id'] for rec in recs]\n",
    "\n",
    "        if not recommended_games:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        relevant_games = test_user_likes[user_id]\n",
    "        if len(relevant_games) > 0:\n",
    "            metrics = calculate_metrics(recommended_games, relevant_games, k)\n",
    "            for metric, value in metrics.items():\n",
    "                metrics_sum[metric] += value\n",
    "            total_users += 1\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    avg_metrics = {\n",
    "        metric: value/total_users if total_users > 0 else 0 \n",
    "        for metric, value in metrics_sum.items()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCollaborative Filtering Metrics:\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"{metric.upper()}@{k}: {value:.4f}\")\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hybrid_system(test_df, similarity_matrix, games_df, recommendations_df, k=5, \n",
    "                         content_weight=0.6, collab_weight=0.4):\n",
    "\n",
    "    metrics_sum = {'precision': 0, 'recall': 0, 'f1': 0, 'ndcg': 0}\n",
    "    total_users = 0\n",
    "    \n",
    "    # Get test set ground truth\n",
    "    test_user_likes = defaultdict(set)\n",
    "    for _, row in test_df[test_df['is_recommended'] == 1].iterrows():\n",
    "        test_user_likes[row['user_id']].add(row['app_id'])\n",
    "    \n",
    "    # Only evaluate users with sufficient test data\n",
    "    valid_users = [user_id for user_id, games in test_user_likes.items() if len(games) >= 0]\n",
    "    \n",
    "    for user_id in tqdm(valid_users, desc=f'Hybrid Evaluation (w_content={content_weight:.1f})'):\n",
    "        # Get a random game from user's liked games in training set\n",
    "        user_train_games = recommendations_df[\n",
    "            (recommendations_df['user_id'] == user_id) & \n",
    "            (recommendations_df['is_recommended'] == 1)\n",
    "        ]['app_id'].tolist()\n",
    "        \n",
    "        if not user_train_games:\n",
    "            continue\n",
    "            \n",
    "        input_game = random.choice(user_train_games)\n",
    "        relevant_games = test_user_likes[user_id]\n",
    "        \n",
    "        try:\n",
    "            recommendations = get_hybrid_recommendations(\n",
    "                user_id=user_id,\n",
    "                game_id=input_game,\n",
    "                similarity_matrix=similarity_matrix,\n",
    "                games_df=games_df,\n",
    "                recommendations_df=recommendations_df,\n",
    "                n_recommendations=k,\n",
    "                content_weight=content_weight,\n",
    "                collab_weight=collab_weight\n",
    "            )\n",
    "            \n",
    "            rec_ids = [rec['game_id'] for rec in recommendations]\n",
    "            metrics = calculate_metrics(rec_ids, relevant_games, k)\n",
    "            \n",
    "            for metric, value in metrics.items():\n",
    "                metrics_sum[metric] += value\n",
    "            total_users += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error for user {user_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_metrics = {\n",
    "        metric: value/total_users if total_users > 0 else 0 \n",
    "        for metric, value in metrics_sum.items()\n",
    "    }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "def tune_hybrid_weights(val_df, similarity_matrix, games_df, recommendations_df, k=5):\n",
    "\n",
    "    best_metrics = {'precision': 0, 'recall': 0, 'f1': 0, 'ndcg': 0}\n",
    "    best_weights = (0.5, 0.5)\n",
    "    results = []\n",
    "    \n",
    "    weight_options = np.arange(0.1, 1.0, 0.1)\n",
    "    \n",
    "    print(\"Starting grid search for hybrid weights...\")\n",
    "    pbar_outer = tqdm(total=len(weight_options), desc='Grid Search Progress', position=0)\n",
    "    \n",
    "    for content_weight in weight_options:\n",
    "        collab_weight = 1 - content_weight\n",
    "        \n",
    "        metrics = evaluate_hybrid_system(\n",
    "            val_df, similarity_matrix, games_df, recommendations_df,\n",
    "            k=k, content_weight=content_weight, collab_weight=collab_weight\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'content_weight': content_weight,\n",
    "            'collab_weight': collab_weight,\n",
    "            **metrics\n",
    "        })\n",
    "        \n",
    "        if metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = metrics\n",
    "            best_weights = (content_weight, collab_weight)\n",
    "        \n",
    "        pbar_outer.update(1)\n",
    "    \n",
    "    pbar_outer.close()\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nGrid Search Results:\")\n",
    "    print(results_df)\n",
    "    print(f\"\\nBest weights found: Content={best_weights[0]:.2f}, Collaborative={best_weights[1]:.2f}\")\n",
    "    print(\"\\nBest validation metrics:\")\n",
    "    for metric, value in best_metrics.items():\n",
    "        print(f\"{metric.upper()}@{k}: {value:.4f}\")\n",
    "    \n",
    "    return best_weights, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation of all recommendation systems...\n",
      "\n",
      "1. Evaluating Content-Based System...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9486002804b64db78cf06932228c7e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Content-Based Evaluation:   0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content-Based Metrics:\n",
      "PRECISION@5: 0.0047\n",
      "RECALL@5: 0.0155\n",
      "F1@5: 0.0069\n",
      "NDCG@5: 0.0106\n",
      "\n",
      "2. Evaluating Collaborative Filtering System...\n",
      "Building test set ground truth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2982f0679be44a78c11a4c97fbff122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test data:   0%|          | 0/32744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd6da2b629f49e3876e09ec4455652c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collaborative Evaluation:   0%|          | 0/34975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collaborative Filtering Metrics:\n",
      "PRECISION@5: 0.0084\n",
      "RECALL@5: 0.0416\n",
      "F1@5: 0.0140\n",
      "NDCG@5: 0.0273\n",
      "\n",
      "3. Tuning and Evaluating Hybrid System...\n",
      "Starting grid search for hybrid weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe71911ea794495975b5210978f750a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grid Search Progress:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f371bab27944958cc844a3b9a9a890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.1):   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff44f6c5de1e41f2b153fa429452517c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.2):   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd839997bff643aba607e0cf2331bc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.3):   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5728fefd37a0407fa07980e45804f2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.4):   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c05d61b126a4025b60ebb939f0c1daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.5):   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453a7e96c6df4dd999dca88fc6468608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.6):   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab7df7326a94a69af387e01fb01efdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.7):   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687bb14101d2445391ebbb33273b341c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.8):   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369afbd589a54be2ba289302af9a8c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.9):   0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search Results:\n",
      "   content_weight  collab_weight  precision    recall        f1      ndcg\n",
      "0             0.1            0.9   0.001988  0.003214  0.002337  0.002209\n",
      "1             0.2            0.8   0.002386  0.004887  0.003075  0.003732\n",
      "2             0.3            0.7   0.003976  0.007522  0.005100  0.005326\n",
      "3             0.4            0.6   0.002783  0.004873  0.003257  0.003958\n",
      "4             0.5            0.5   0.002386  0.004639  0.003124  0.003354\n",
      "5             0.6            0.4   0.006362  0.012577  0.008020  0.011407\n",
      "6             0.7            0.3   0.002783  0.004876  0.003361  0.004647\n",
      "7             0.8            0.2   0.006362  0.012894  0.008232  0.012592\n",
      "8             0.9            0.1   0.005169  0.009711  0.006291  0.008328\n",
      "\n",
      "Best weights found: Content=0.80, Collaborative=0.20\n",
      "\n",
      "Best validation metrics:\n",
      "PRECISION@5: 0.0064\n",
      "RECALL@5: 0.0129\n",
      "F1@5: 0.0082\n",
      "NDCG@5: 0.0126\n",
      "\n",
      "4. Final Hybrid System Evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2535fc6441604c8dab987babf5cfa093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hybrid Evaluation (w_content=0.8):   0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results Summary:\n",
      "==================================================\n",
      "\n",
      "PRECISION@5:\n",
      "Content-Based: 0.0047\n",
      "Collaborative: 0.0084\n",
      "Hybrid: 0.0032\n",
      "\n",
      "RECALL@5:\n",
      "Content-Based: 0.0155\n",
      "Collaborative: 0.0416\n",
      "Hybrid: 0.0069\n",
      "\n",
      "F1@5:\n",
      "Content-Based: 0.0069\n",
      "Collaborative: 0.0140\n",
      "Hybrid: 0.0043\n",
      "\n",
      "NDCG@5:\n",
      "Content-Based: 0.0106\n",
      "Collaborative: 0.0273\n",
      "Hybrid: 0.0067\n",
      "\n",
      "Best Hybrid Configuration:\n",
      "Content Weight: 0.80\n",
      "Collaborative Weight: 0.20\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting comprehensive evaluation of all recommendation systems...\")\n",
    "\n",
    "print(\"\\n1. Evaluating Content-Based System...\")\n",
    "content_metrics = evaluate_content_based_system(test_df, similarity_matrix, games_df, k=5)\n",
    "\n",
    "print(\"\\n2. Evaluating Collaborative Filtering System...\")\n",
    "collab_metrics = evaluate_collaborative_system(train_df, test_df, games_df, k=5)\n",
    "\n",
    "print(\"\\n3. Tuning and Evaluating Hybrid System...\")\n",
    "best_weights, best_val_metrics = tune_hybrid_weights(val_df, similarity_matrix, games_df, recommendations_df, k=5)\n",
    "\n",
    "print(\"\\n4. Final Hybrid System Evaluation...\")\n",
    "content_weight, collab_weight = best_weights\n",
    "hybrid_metrics = evaluate_hybrid_system(\n",
    "    test_df, similarity_matrix, games_df, recommendations_df,\n",
    "    k=5, content_weight=content_weight, collab_weight=collab_weight\n",
    ")\n",
    "\n",
    "# Print final comparison\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "print(\"=\" * 50)\n",
    "metrics = ['precision', 'recall', 'f1', 'ndcg']\n",
    "systems = {\n",
    "    'Content-Based': content_metrics,\n",
    "    'Collaborative': collab_metrics,\n",
    "    'Hybrid': hybrid_metrics\n",
    "}\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"\\n{metric.upper()}@5:\")\n",
    "    for system, results in systems.items():\n",
    "        print(f\"{system}: {results[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nBest Hybrid Configuration:\")\n",
    "print(f\"Content Weight: {content_weight:.2f}\")\n",
    "print(f\"Collaborative Weight: {collab_weight:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
